{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SchoolBudgetsCaseStudy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5smhfI3ig0uoxhalJnr0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mossytreesandferns/DataCampHomework/blob/master/SchoolBudgetsCaseStudy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgzsazCQ_Gt8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b496f924-7fc2-4147-edc3-1e25dd8e8bbd"
      },
      "source": [
        "#Mount my drive- run the code, go to the link, accept.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2waedyJ_PKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0df80546-7a08-4b3e-b556-30b005425b61"
      },
      "source": [
        "#Change working directory to make it easier to access the files\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/DCTreeModels/\")\n",
        "os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/DCTreeModels'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvLD1681_PVk",
        "colab_type": "text"
      },
      "source": [
        "# Exploring the raw data\n",
        "### Introducing the challenge\n",
        "Classification, supervised, predicting probabilities of each label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfUk6B1cOo7V",
        "colab_type": "text"
      },
      "source": [
        "### Loading the data\n",
        "loading data, df.info(), df.describe(), looking at non-null values, data types, with df.info(), "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JPdnEIlQXKC",
        "colab_type": "text"
      },
      "source": [
        "### Looking at the datatypes\n",
        "Encoding categorical variables, changing object (string) columns to category (changed to numeric values under the hood), use get_dummies to show numerical values, lambda functions for concerting columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUVhj6wY_PjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dtypes\n",
        "df.dtypes.value_counts()\n",
        "\n",
        "# Define the lambda function: categorize_label\n",
        "categorize_label = lambda x: x.astype('category')\n",
        "\n",
        "# Convert df[LABELS] to a categorical type\n",
        "df[LABELS] = df[LABELS].apply(categorize_label, axis=0)\n",
        "\n",
        "# Print the converted dtypes\n",
        "print(df[LABELS].dtypes)\n",
        "\n",
        "# Import matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate number of unique values for each label: num_unique_labels\n",
        "num_unique_labels = df[LABELS].apply(pd.Series.nunique)\n",
        "\n",
        "# Plot number of unique values for each label\n",
        "num_unique_labels.plot(kind='bar')\n",
        "\n",
        "# Label the axes\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Number of unique values')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIKAzr69VnWk",
        "colab_type": "text"
      },
      "source": [
        "### How do we measure success?\n",
        "Use logloss function to minimize error when accuracy isn't the right metric(spam email problem), used with logistic regression problems, imbalance of categories in label columns, loss function is measure of error, logloss measure is smaller when confidence (probability of prediction) is lower because higher penalty built in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC4z-zx8eqx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute and print log loss for 1st case\n",
        "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
        "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss)) \n",
        "\n",
        "# Compute log loss for 2nd case\n",
        "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
        "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss)) \n",
        "\n",
        "# Compute and print log loss for 3rd case\n",
        "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
        "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss)) \n",
        "\n",
        "# Compute and print log loss for 4th case\n",
        "wrong_confident_loss = compute_log_loss(wrong_confident,actual_labels)\n",
        "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss)) \n",
        "\n",
        "# Compute and print log loss for actual labels\n",
        "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
        "print(\"Log loss, actual labels: {}\".format(actual_labels_loss)) \n",
        "\n",
        "Log loss, correct and confident: 0.05129329438755058\n",
        "Log loss, correct and not confident: 0.4307829160924542\n",
        "Log loss, wrong and not confident: 1.049822124498678\n",
        "Log loss, wrong and confident: 2.9957322735539904\n",
        "Log loss, actual labels: 9.99200722162646e-15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv0OhSWDe5Hn",
        "colab_type": "text"
      },
      "source": [
        "# Creating a simple first model\n",
        "### It's time to build a model\n",
        "Build a simple model first, process numeric columns first, train on each numeric column separately, save predictions, if there are a lot of labels in a supervised models, some can escape being split correctly for training and testing, must use a splitting function that distributes labels correctly, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XZqhWaBe-Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DC provided label splitting code \n",
        "\n",
        "from warnings import warn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
        "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
        "        the indices for a sample of size `size` if\n",
        "        `size` > 1 or `size` * len(y) if size =< 1.\n",
        "        The sample is guaranteed to have > `min_count` of\n",
        "        each label.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n",
        "            raise ValueError()\n",
        "    except (TypeError, ValueError):\n",
        "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
        "\n",
        "    if (y.sum(axis=0) < min_count).any():\n",
        "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
        "\n",
        "    if size <= 1:\n",
        "        size = np.floor(y.shape[0] * size)\n",
        "\n",
        "    if y.shape[1] * min_count > size:\n",
        "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
        "        warn(msg.format(y.shape[1] * min_count, size))\n",
        "        size = y.shape[1] * min_count\n",
        "\n",
        "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
        "\n",
        "    if isinstance(y, pd.DataFrame):\n",
        "        choices = y.index\n",
        "        y = y.values\n",
        "    else:\n",
        "        choices = np.arange(y.shape[0])\n",
        "\n",
        "    sample_idxs = np.array([], dtype=choices.dtype)\n",
        "\n",
        "    # first, guarantee > min_count of each label\n",
        "    for j in range(y.shape[1]):\n",
        "        label_choices = choices[y[:, j] == 1]\n",
        "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
        "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
        "\n",
        "    sample_idxs = np.unique(sample_idxs)\n",
        "\n",
        "    # now that we have at least min_count of each, we can just random sample\n",
        "    sample_count = int(size - sample_idxs.shape[0])\n",
        "\n",
        "    # get sample_count indices from remaining choices\n",
        "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
        "    remaining_sampled = rng.choice(remaining_choices,\n",
        "                                   size=sample_count,\n",
        "                                   replace=False)\n",
        "\n",
        "    return np.concatenate([sample_idxs, remaining_sampled])\n",
        "\n",
        "\n",
        "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
        "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
        "        classes in the binary matrix `labels` are represented at\n",
        "        least `min_count` times.\n",
        "    \"\"\"\n",
        "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
        "    return df.loc[idxs]\n",
        "\n",
        "\n",
        "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
        "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
        "        returns (X_train, X_test, Y_train, Y_test) where all\n",
        "        classes in Y are represented at least `min_count` times.\n",
        "    \"\"\"\n",
        "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
        "\n",
        "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
        "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
        "\n",
        "    test_set_mask = index.isin(test_set_idxs)\n",
        "    train_set_mask = ~test_set_mask\n",
        "\n",
        "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89kg8gyWYluE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the new DataFrame: numeric_data_only\n",
        "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
        "\n",
        "# Get labels and convert to dummy variables: label_dummies\n",
        "label_dummies = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only, label_dummies,\n",
        "                                                               size=0.2, \n",
        "                                                               seed=123)\n",
        "\n",
        "# Print the info\n",
        "print(\"X_train info:\")\n",
        "print(X_train.info())\n",
        "print(\"\\nX_test info:\")  \n",
        "print(X_test.info())\n",
        "print(\"\\ny_train info:\")  \n",
        "print(y_train.info())\n",
        "print(\"\\ny_test info:\")  \n",
        "print(y_test.info()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPMR73QRdTMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Create the DataFrame: numeric_data_only\n",
        "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
        "\n",
        "# Get labels and convert to dummy variables: label_dummies\n",
        "label_dummies = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
        "                                                               label_dummies,\n",
        "                                                               size=0.2, \n",
        "                                                               seed=123)\n",
        "\n",
        "# Instantiate the classifier: clf\n",
        "clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi725byudYy5",
        "colab_type": "text"
      },
      "source": [
        "### Making predictions\n",
        "Read in holdout data provided by imaginary data science competition, preprocess for missing values as before, predict proba on holdout data, submit preditions in csv form to competition, turn into dataframe then to_csv,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1OdhMrmdaos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the classifier: clf\n",
        "clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "# Fit it to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Load the holdout data: holdout\n",
        "holdout = pd.read_csv('HoldoutData.csv', index_col=0)\n",
        "\n",
        "# Generate predictions: predictions\n",
        "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWmnln0Im9vH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate predictions: predictions\n",
        "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
        "\n",
        "# Format predictions in DataFrame: prediction_df\n",
        "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns, index=holdout.index, data=predictions)\n",
        "\n",
        "\n",
        "# Save prediction_df to csv\n",
        "prediction_df.to_csv('predictions.csv')\n",
        "\n",
        "# Submit the predictions for scoring: score\n",
        "score = score_submission('predictions.csv')\n",
        "\n",
        "# Print score\n",
        "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCsfV11ynbEo",
        "colab_type": "text"
      },
      "source": [
        "### A very brief introduction to NLP\n",
        "tokenizing, n-grams, splitting on whitespace and punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBde9QTmpqT0",
        "colab_type": "text"
      },
      "source": [
        "### Representing text numerically\n",
        "CountVectorizer() - tokeinizes, counts each word in text unit.  Regex basic string-splitting token, fillna values, instantiate CountVectorizer()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly1iTzhTr2N7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Fill missing values in df.Position_Extra\n",
        "df.Position_Extra.fillna('', inplace=True)\n",
        "\n",
        "# Instantiate the CountVectorizer: vec_alphanumeric\n",
        "vec_alphanumeric = CountVectorizer(TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit to the data\n",
        "vec_alphanumeric.fit(df.Position_Extra)\n",
        "\n",
        "# Print the number of tokens and first 15 tokens\n",
        "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
        "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
        "print(vec_alphanumeric.get_feature_names()[:15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWJVCZ3yr2m4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define combine_text_columns()\n",
        "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
        "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
        "    \n",
        "    # Drop non-text columns that are in the df\n",
        "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
        "    text_data = data_frame.drop(to_drop, axis=1)\n",
        "    \n",
        "    # Replace nans with blanks\n",
        "    text_data.fillna('',inplace=True)\n",
        "    \n",
        "    # Join all text items in a row that have a space in between\n",
        "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhRDY5zOvJUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the basic token pattern\n",
        "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
        "\n",
        "# Create the alphanumeric token pattern\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate basic CountVectorizer: vec_basic\n",
        "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
        "\n",
        "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
        "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Create the text vector\n",
        "text_vector = combine_text_columns(df)\n",
        "\n",
        "# Fit and transform vec_basic\n",
        "vec_basic.fit_transform(text_vector)\n",
        "\n",
        "# Print number of tokens of vec_basic\n",
        "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
        "\n",
        "# Fit and transform vec_alphanumeric\n",
        "vec_alphanumeric.fit(text_vector)\n",
        "\n",
        "# Print number of tokens of vec_alphanumeric\n",
        "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HABLpFNKvYzg",
        "colab_type": "text"
      },
      "source": [
        "# Improving your model\n",
        "### Pipelines, feature & text preprocessing\n",
        "Using pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wpQPvTxve5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Import other necessary modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Split and select numeric data only, no nans \n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=22)\n",
        "\n",
        "# Instantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30EDZWVpAsF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Imputer object\n",
        "from sklearn.preprocessing import Imputer\n",
        "\n",
        "# Create training and test sets using only numeric data\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=456)\n",
        "\n",
        "# Insantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "        ('imp', Imputer()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGx40tYlAwbq",
        "colab_type": "text"
      },
      "source": [
        "### Text features and feature unions\n",
        "pipeline steps for text preprocessing and numeric features can't follow each other <br>\n",
        "FunctionTransformer() and FeatureUnion() lets you work with both."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4WwmdoOAzRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Split out only the text data\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=456)\n",
        "\n",
        "# Instantiate Pipeline object: pl\n",
        "pl = Pipeline([\n",
        "        ('vec', CountVectorizer()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - just text data: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngILPp9bIMjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import FunctionTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Obtain the text data: get_text_data\n",
        "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
        "\n",
        "# Obtain the numeric data: get_numeric_data\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
        "\n",
        "# Fit and transform the text data: just_text_data\n",
        "just_text_data = get_text_data.fit_transform(sample_df)\n",
        "\n",
        "# Fit and transform the numeric data: just_numeric_data\n",
        "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
        "\n",
        "# Print head to check results\n",
        "print('Text Data')\n",
        "print(just_text_data.head())\n",
        "print('\\nNumeric Data')\n",
        "print(just_numeric_data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTt_rs33IMn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import FeatureUnion\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "# Split using ALL data in sample_df\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],\n",
        "                                                    pd.get_dummies(sample_df['label']), \n",
        "                                                    random_state=22)\n",
        "\n",
        "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
        "process_and_join_features = FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )\n",
        "\n",
        "# Instantiate nested pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', process_and_join_features),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "\n",
        "# Fit pl to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on sample data - all data: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRgfdyvxJ8M1",
        "colab_type": "text"
      },
      "source": [
        "### Choosing a classification model\n",
        "Remember that an outside function combines columns is necessary for processing more than one text column. <br>\n",
        "To change algorithms just change the algo imput into OneVsRestClassifier() when instantiating pl."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLM3SGZ9LoFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import FunctionTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Get the dummy encoding of the labels\n",
        "dummy_labels = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Get the columns that are features in the original df\n",
        "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
        "                                                               dummy_labels,\n",
        "                                                               0.2, \n",
        "                                                               seed=123)\n",
        "\n",
        "# Preprocess the text data: get_text_data\n",
        "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
        "\n",
        "# Preprocess the numeric data: get_numeric_data\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8iIRN2ILoMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Complete the pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RpGVYCpLoJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import random forest classifer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Edit model step in pipeline\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('clf', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVCXKHjCKLfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Add model step to pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer())\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('clf', RandomForestClassifier(n_estimators=15))\n",
        "    ])\n",
        "\n",
        "# Fit to the training data\n",
        "pl.fit(X_train, y_train)\n",
        "\n",
        "# Compute and print accuracy\n",
        "accuracy = pl.score(X_test, y_test)\n",
        "print(\"\\nAccuracy on budget dataset: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w3GuUkzPExx",
        "colab_type": "text"
      },
      "source": [
        "# Learning from the experts\n",
        "### Processing\n",
        "Tokenize on punctuation, include unigrams and bigrams to include important phrases. CountVectorizer(token_patterm=TOKENS_ALPHANUMERIC, ngrams_range=(1,2))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1TVihDuPG-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Import classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Import other preprocessing modules\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "\n",
        "# Select 300 best features\n",
        "chi_k = 300\n",
        "\n",
        "# Import functional utilities\n",
        "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "# Perform preprocessing\n",
        "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1, 2)\n",
        "                                                   )),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSF8vUJtPcEM",
        "colab_type": "text"
      },
      "source": [
        "### Learning from the experts A stats trick\n",
        "Interaction term for B1x + B2y is B3xy, which describes when tokens appear together (interact)<br>\n",
        "PolynomialFeatures(), and SparseInteractions()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJWZ1gcsPg6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                   ngram_range=(1, 2))),  \n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh7_dg_ER4q5",
        "colab_type": "text"
      },
      "source": [
        "### Learning from teh expert A winning model\n",
        "Hashing trick: Increases memory efficiency by mapping tokens to numbers where tokens are keys and numbers are values. Multiple tokens can map to the same value.  It is a method of dimensionality reduction for word features. <br>  \n",
        "HashingVectorizer(norm=None, non_negative=True, otken_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2))  <br>\n",
        "<br>\n",
        "Summary: NLP: tokens with range of n grams, adding interaction terms, hash trick. With logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-4ORjp4R75-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23ba818b-8632-43d9-a561-382f2f30ba48"
      },
      "source": [
        "d = {'a': 1, 'b': 2}\n",
        "d"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 1, 'b': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7R1vA6ZUuuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27923ff0-4cf9-44b1-88cc-a3797dbf1690"
      },
      "source": [
        "d['b'] = 3\n",
        "d"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 1, 'b': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpPwRqi9UzUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Get text data: text_data\n",
        "text_data = combine_text_columns(X_train)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
        "\n",
        "# Instantiate the HashingVectorizer: hashing_vec\n",
        "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit and transform the Hashing Vectorizer\n",
        "hashed_text = hashing_vec.fit_transform(text_data)\n",
        "\n",
        "# Create DataFrame and print the head\n",
        "hashed_df = pd.DataFrame(hashed_text.data)\n",
        "print(hashed_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdZDZwP9WDFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the hashing vectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Instantiate the winning model pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                     non_negative=True, norm=None, binary=False,\n",
        "                                                     ngram_range=(1,2))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}